{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "933bd0d4-5c70-41d5-b09b-fcebefb94dbd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "335d02df-14bc-4a5b-b61e-642915eb5363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seed = 2000\n",
    "protected = False\n",
    "sam = False\n",
    "import_cols = [\n",
    "    'RESEARCH_AND_DEVELOPMENT',\n",
    "    'DOMESTIC_SHELTER',\n",
    "    'TYPE_OF_SET_ASIDE',\n",
    "    'SOLICITATION_ID',\n",
    "    'CONSTRUCTION_FIRM',\n",
    "    'CO_BUS_SIZE_DETERMINATION',\n",
    "    'CAGE_CODE',\n",
    "    'VETERAN_OWNED_FLAG',\n",
    "    'CORP_ENTITY_NOT_TAX_EXEMPT',\n",
    "    'FUNDING_DEPARTMENT_ID',\n",
    "    # 'FUNDING_AGENCY_NAME',\n",
    "    'FUNDING_AGENCY_ID',\n",
    "    # 'FUNDING_OFFICE_NAME',\n",
    "    'FUNDING_OFFICE_ID',\n",
    "    'SERVICE_PROVIDER',\n",
    "    'PRODUCT_OR_SERVICE_TYPE',\n",
    "    # 'MODIFICATION_NUMBER',\n",
    "    # 'PIID',\n",
    "    'FOUNDATION',\n",
    "    # 'EVALUATED_PREFERENCE',\n",
    "    'SRDVOB_FLAG',\n",
    "    'CORP_ENTITY_TAX_EXEMPT',\n",
    "    'MANUFACTURER_OF_GOODS',\n",
    "    'VENDOR_ADDRESS_COUNTRY_NAME',\n",
    "    'VENDOR_ADDRESS_ZIP_CODE',\n",
    "    'SDB',\n",
    "    'VETERINARY_HOSPITAL',\n",
    "    'COMMUNITY_CORP_OWNED_FIRM',\n",
    "    'DOT_CERTIFIED_DISADV_BUS',\n",
    "    'PRINCIPAL_NAICS_CODE',\n",
    "    'EDUCATIONAL_INSTITUTION_FLAG',\n",
    "    'LIMITED_LIABILITY_CORPORATION',\n",
    "    'EXTENT_COMPETED',\n",
    "    'FEDERALLY_FUNDED_R_AND_D_CORP',\n",
    "    'SOLE_PROPREITORSHIP',\n",
    "    'WOMEN_OWNED_FLAG',\n",
    "    'ARCHITECTURE_AND_ENGINEERING',\n",
    "    'HISPANIC_SERVICING_INSTITUTION',\n",
    "    # 'IDV_PIID',\n",
    "    'PLACE_OF_MANUFACTURE',\n",
    "    # 'IDV_EXTENT_COMPETED',\n",
    "    'AWARD_FISCAL_YEAR',\n",
    "    # 'IDV_SIGNED_DATE',\n",
    "    'FIRM_8A_FLAG',\n",
    "    'SMALL_AGRICULTURAL_COOPERATIVE',\n",
    "    'PARTNERSHIP_OR_LLP',\n",
    "    'DOLLARS_OBLIGATED',\n",
    "    # 'IDV_NUMBER_OF_OFFERS',\n",
    "    'FOR_PROFIT_ORGANIZATION',\n",
    "    # 'AWARD_OR_IDV',\n",
    "    'FIRM8A_JOINT_VENTURE',\n",
    "    # 'IDV_CONTRACTING_AGENCY_ID',\n",
    "]\n",
    "if protected:\n",
    "    import_cols = import_cols + ['ANNUAL_REVENUE', 'NUMBER_OF_EMPLOYEES']\n",
    "\n",
    "\n",
    "import_years = [2019]#, 2020, 2021, 2022, 2023]\n",
    "min_class_size = 8\n",
    "\n",
    "model_cols = [\n",
    "    'RESPONSE',\n",
    "    # 'FUNDING_AGENCY_ID',\n",
    "    'RESEARCH_AND_DEVELOPMENT',\n",
    "    'DOMESTIC_SHELTER',\n",
    "    'TYPE_OF_SET_ASIDE',\n",
    "    'CONSTRUCTION_FIRM',\n",
    "    'VETERAN_OWNED_FLAG',\n",
    "    'CORP_ENTITY_NOT_TAX_EXEMPT',\n",
    "    'SERVICE_PROVIDER',\n",
    "    'PRODUCT_OR_SERVICE_TYPE',\n",
    "    # 'MODIFICATION_NUMBER',\n",
    "    # 'PIID',\n",
    "    'FOUNDATION',\n",
    "    # 'EVALUATED_PREFERENCE',\n",
    "    'SRDVOB_FLAG',\n",
    "    'CORP_ENTITY_TAX_EXEMPT',\n",
    "    'MANUFACTURER_OF_GOODS',\n",
    "    'VENDOR_ADDRESS_ZIP_CODE',\n",
    "    #'VENDOR_STATE', #FEATURE ENGINEERED BELOW\n",
    "    'SDB',\n",
    "    'VETERINARY_HOSPITAL',\n",
    "    'COMMUNITY_CORP_OWNED_FIRM',\n",
    "    'DOT_CERTIFIED_DISADV_BUS',\n",
    "    'PRINCIPAL_NAICS_CODE',\n",
    "    'EDUCATIONAL_INSTITUTION_FLAG',\n",
    "    'LIMITED_LIABILITY_CORPORATION',\n",
    "    'FEDERALLY_FUNDED_R_AND_D_CORP',\n",
    "    'SOLE_PROPREITORSHIP',\n",
    "    'WOMEN_OWNED_FLAG',\n",
    "    'ARCHITECTURE_AND_ENGINEERING',\n",
    "    'HISPANIC_SERVICING_INSTITUTION',\n",
    "    # 'IDV_PIID',\n",
    "    'PLACE_OF_MANUFACTURE_CLASS',\n",
    "    # 'IDV_EXTENT_COMPETED',\n",
    "    # 'AWARD_FISCAL_YEAR',\n",
    "    # 'IDV_SIGNED_DATE',\n",
    "    'FIRM_8A_FLAG',\n",
    "    'SMALL_AGRICULTURAL_COOPERATIVE',\n",
    "    'PARTNERSHIP_OR_LLP',\n",
    "    # 'IDV_NUMBER_OF_OFFERS',\n",
    "    'FOR_PROFIT_ORGANIZATION',\n",
    "    # 'AWARD_OR_IDV',\n",
    "    'FIRM8A_JOINT_VENTURE',\n",
    "    # 'IDV_CONTRACTING_AGENCY_ID',\n",
    "    'CONTRACTS_PER_YEAR'\n",
    "]\n",
    "if protected:\n",
    "    model_cols = model_cols + ['ANNUAL_REVENUE', 'NUMBER_OF_EMPLOYEES']\n",
    "\n",
    "min_importance = 0.015 #at 0.015 we finally get zip, \n",
    "col_tree_depth = 4\n",
    "hard = False\n",
    "\n",
    "n_trees = 100\n",
    "max_depth = 7\n",
    "top_n_classes = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b373c95-bd81-4007-8c74-96b6cf85ae77",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fda2e1e1-c11d-4419-80b6-adbabcbcc794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_dataset(import_cols, years, sam=True):\n",
    "    \"\"\"\n",
    "    Imports, cleans, and joins our data with specified columns and years\n",
    "    Inputs: \n",
    "        import_cols (list [str] of column names)\n",
    "        years (list [int] of years to import)\n",
    "        sam (bool of whether to merge with sam dataset)\n",
    "    Output:\n",
    "        Cleaned, filtered, and joined dataframe\n",
    "    \"\"\"\n",
    "    if sam:\n",
    "        SAM = pd.read_csv('SAM.CSV') #imports SAM df\n",
    "    \n",
    "    year_dfs = []\n",
    "    for year in years:\n",
    "        if protected: #whether we're using protected columns or not\n",
    "            temp_df = pd.read_parquet('fy' + str(year) + '.parquet', columns=import_cols)\n",
    "        else:\n",
    "            temp_df = pd.read_parquet(str(year) + '.parquet', columns=import_cols) #import year's data\n",
    "        \n",
    "        temp_df = temp_df[temp_df['CO_BUS_SIZE_DETERMINATION'] == \"SMALL BUSINESS\"] #filter for small business\n",
    "        temp_df = temp_df[temp_df['VENDOR_ADDRESS_COUNTRY_NAME'] == \"UNITED STATES\"] #filter for US\n",
    "        temp_df = temp_df[temp_df['EXTENT_COMPETED'].isin([\"A\", \"D\", \"E\", \"CDO\"])] #filter for competition\n",
    "        \n",
    "        # temp_df['DOLLARS_OBLIGATED'] = pd.to_numeric(temp_df['DOLLARS_OBLIGATED'], errors='coerce') #make numeric\n",
    "        \n",
    "        if sam:\n",
    "            temp_m = pd.merge(temp_df, SAM, on=\"CAGE_CODE\", how=\"inner\") #merge with SAM\n",
    "        else:\n",
    "            temp_m = temp_df\n",
    "        \n",
    "        # idx = temp_m.groupby(['SOLICITATION_ID','CAGE_CODE'])['DOLLARS_OBLIGATED'].idxmax() #find initial contract win\n",
    "        # temp_m = temp_m.loc[idx] #filter to initial contract win\n",
    "        \n",
    "        temp_m = temp_m[temp_m['DOLLARS_OBLIGATED'] > 0] #filter DOLLARS_OBLIGATED\n",
    "        \n",
    "        print(f'{year} shape: {temp_m.shape}')\n",
    "        year_dfs.append(temp_m) #append year dataset to list of year datasets\n",
    "    \n",
    "    merged_df = pd.concat(year_dfs, ignore_index=True) #merge all years\n",
    "    \n",
    "    for df in year_dfs:\n",
    "        del df\n",
    "    del year_dfs #delete the individual dfs from memory\n",
    "    \n",
    "    idx = merged_df.groupby(['SOLICITATION_ID','CAGE_CODE'])['DOLLARS_OBLIGATED'].idxmax() #find initial contracts\n",
    "    filtered_merged_df = merged_df.loc[idx] #filter to initial contract\n",
    "    \n",
    "    print(f'total shape: {filtered_merged_df.shape}')\n",
    "    \n",
    "    #place of manufacture conversion\n",
    "    def convert_place_of_manufacture(value):\n",
    "        if value == 'D':\n",
    "            return 'YES' #manufactured in US\n",
    "        elif value == 'C':\n",
    "            return 'NO' #not manufactured in US\n",
    "        elif value in ['N/A', 'A', 'G', 'E', 'H', 'L', 'J', 'F', 'K', 'B', 'I']:\n",
    "            return 'NONE'\n",
    "        else:\n",
    "            return 'NONE' #N/A (provides a service or doesn't qualify fully)\n",
    "    \n",
    "    #clean up individual columns\n",
    "    filtered_merged_df['FUNDING_DEPARTMENT_ID'] = filtered_merged_df['FUNDING_DEPARTMENT_ID'].str.strip() #clean dept ID\n",
    "    if protected:\n",
    "        filtered_merged_df['ANNUAL_REVENUE'] = filtered_merged_df['ANNUAL_REVENUE'].astype(float)\n",
    "    # filtered_merged_df['IDV_PIID'] = filtered_merged_df['IDV_PIID'].str.strip() #clean IDV PIID\n",
    "    # filtered_merged_df['PIID'] = filtered_merged_df['PIID'].str.strip() #clean PIID\n",
    "    filtered_merged_df['PLACE_OF_MANUFACTURE_CLASS'] = filtered_merged_df['PLACE_OF_MANUFACTURE'].apply(convert_place_of_manufacture) #clean PLACE_OF_MANUFACTURE\n",
    "    filtered_merged_df['VENDOR_ADDRESS_ZIP_CODE'] = filtered_merged_df['VENDOR_ADDRESS_ZIP_CODE'].astype(str).str[:5] #clean ZIP to 5-digit\n",
    "    filtered_merged_df = filtered_merged_df[filtered_merged_df['VENDOR_ADDRESS_ZIP_CODE'].str.len()==5]\n",
    "    filtered_merged_df['TYPE_OF_SET_ASIDE'] = filtered_merged_df['TYPE_OF_SET_ASIDE'].fillna('NONE') #assume NA = NONE\n",
    "    \n",
    "    filtered_merged_df = filtered_merged_df.dropna(subset=filtered_merged_df.columns.difference(['PLACE_OF_MANUFACTURE'])) #remove rows with NAs here\n",
    "    \n",
    "    print(f'total filtered shape: {filtered_merged_df.shape}')\n",
    "    \n",
    "    return filtered_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f748019-e9bf-4064-9418-703752e281d9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 shape: (1772611, 41)\n",
      "total shape: (35321, 41)\n",
      "total filtered shape: (35321, 42)\n"
     ]
    }
   ],
   "source": [
    "df0 = import_dataset(import_cols, import_years, sam=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdb9704-2951-4e3a-9ce3-b38802351ad6",
   "metadata": {},
   "source": [
    "## Create Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3fbdd7d-4360-4382-a9cd-2481e31a80ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def response_var(df, min_class_size=1):\n",
    "    \"\"\"\n",
    "    Creates a new response variable by splitting the top_n most common agencies into their respective offices,\n",
    "    then filter the dataframe so only classes with at least min_class_size rows remain\n",
    "    Inputs:\n",
    "        df (dataframe output from import_dataset())\n",
    "        min_class_size (integer of minimum # of observations a class can have to be a valid response)\n",
    "    Outputs:\n",
    "        dataframe with additional RESPONSE column, filtered\n",
    "    \"\"\"\n",
    "    df['RESPONSE'] = df['FUNDING_OFFICE_ID']\n",
    "    \n",
    "    class_sizes = df['RESPONSE'].value_counts()\n",
    "    classes_to_keep = class_sizes[class_sizes >= min_class_size].index\n",
    "    df = df[df['RESPONSE'].isin(classes_to_keep)]\n",
    "    \n",
    "    print(f\"{df['RESPONSE'].nunique()} unique response classes\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b837548c-d4a3-4689-a2c9-4f2322f8ac72",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "870 unique response classes\n"
     ]
    }
   ],
   "source": [
    "df = response_var(df0, min_class_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdcd452-b7ee-47fe-982f-224e535e13a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Perform any Feature Engineering Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f7fab84-9441-4aee-8089-a9d44f1fb89e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[:,'PRINCIPAL_NAICS_CODE'] = df['PRINCIPAL_NAICS_CODE'].str[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bedb72bf-7df4-4f54-a0c6-fa95ea44abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_df = pd.read_csv('zip_code_database.csv', converters={'zip': str})\n",
    "# zcdb = pd.Series(zip_df['state'].values,index=zip_df['zip']).to_dict()\n",
    "# df.loc[:,'VENDOR_STATE'] = df[\"VENDOR_ADDRESS_ZIP_CODE\"].map(zcdb, na_action='ignore')\n",
    "# df = df[~df['VENDOR_STATE'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bcad5d4-b52b-4cad-b6f6-4b3282728aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[:,'VENDOR_ADDRESS_ZIP_CODE'] = df['VENDOR_ADDRESS_ZIP_CODE'].str[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc600b75-2491-42dd-9732-74ba93b2998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "contracts_per_year = df.groupby(['CAGE_CODE', 'AWARD_FISCAL_YEAR']).size().to_frame().reset_index()\n",
    "contracts_per_year.columns = list(contracts_per_year.columns[:2]) + ['CONTRACTS_PER_YEAR']\n",
    "df = df.merge(contracts_per_year, on=['CAGE_CODE', 'AWARD_FISCAL_YEAR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dcd052-c6d1-419d-bae5-701c00286fcc",
   "metadata": {},
   "source": [
    "## Model Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae0f8867-89b2-4e4e-926d-e3b7c32becf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, model_cols, train_test=True, train_size=0.8, scale_quant=True):\n",
    "    \"\"\"\n",
    "    Preprocesses the dataframe appropriately after creating the response variable and after feature engineering\n",
    "    Input:\n",
    "        df (dataframe output from response_var())\n",
    "        model_cols (list [str] of cols to use to model)\n",
    "        train_test (bool of whether to split the data into train/test groups)\n",
    "        train_size (float [0, 1] of proportion of training data to whole data)\n",
    "        scale_quant (bool of whether to scale quantitative variables)\n",
    "        \n",
    "    Output:\n",
    "        X (df of X values to be used in modeling, dummy encoded & scaled)\n",
    "        y (series of y values for modeling, label encoded)\n",
    "    \"\"\"\n",
    "    Xy = df[model_cols] #select only the modeling columns\n",
    "    \n",
    "    y = Xy['RESPONSE'] #initialize y\n",
    "    X = Xy.drop('RESPONSE', axis=1) #initialize X\n",
    "    \n",
    "    enc = LabelEncoder()\n",
    "    y = enc.fit_transform(y) #transform y into labeled column\n",
    "    y = pd.DataFrame({'RESPONSE': y,\n",
    "                    'FUNDING_AGENCY_ID': df['FUNDING_AGENCY_ID']})\n",
    "    global class_ids \n",
    "    class_ids = enc.classes_\n",
    "    \n",
    "    categoricals = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    X_cat = pd.get_dummies(X[categoricals]) #one-hot encode categorical columns\n",
    "    \n",
    "    quantitatives = X.columns.difference(categoricals)\n",
    "    if scale_quant and len(quantitatives) > 0:\n",
    "        scaler = StandardScaler()\n",
    "        X_quant = scaler.fit_transform(X[quantitatives]) #scale quantitative columns\n",
    "        X_quant = pd.DataFrame(X_quant, columns=quantitatives)\n",
    "        X_cat = X_cat.reset_index()\n",
    "    else:\n",
    "        X_quant = X[quantitatives]\n",
    "    \n",
    "    X = pd.concat([X_cat, X_quant, df[['FUNDING_AGENCY_ID']]], axis=1).drop('index', axis=1) #combine quant & cat subsets back into one df\n",
    "    \n",
    "    if train_test:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_size, random_state=seed)\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    else:\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a930542d-263e-4d28-bdef-8b875e76605d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = preprocess_data(df, model_cols, train_test=True, train_size=0.8, scale_quant=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306ad8d-dfa9-45e8-b497-64857ed78bba",
   "metadata": {},
   "source": [
    "## Column Selection for Each Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f69466f-a876-4329-b0ca-d13be007d469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Dashboard_Columns.pkl', 'rb') as f:\n",
    "        final_cols = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b242b9c3-77f3-411c-9f23-e63786982d15",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Random Forest & Assess for Each Agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58928ac5-e676-4649-aa38-6f97b312fd54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "951a5e14-4618-40a6-8cda-37607c2b1078",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_forest(X_train, X_test, y_train, y_test, agency_id, columns, top_n_classes, n_trees=1000, max_depth=2):\n",
    "    X_train = X_train[X_train['FUNDING_AGENCY_ID']==agency_id]\n",
    "    X_test = X_test[X_test['FUNDING_AGENCY_ID']==agency_id]\n",
    "    y_train = y_train[y_train['FUNDING_AGENCY_ID']==agency_id].drop('FUNDING_AGENCY_ID', axis=1).to_numpy().ravel()\n",
    "    y_test = y_test[y_test['FUNDING_AGENCY_ID']==agency_id].drop('FUNDING_AGENCY_ID', axis=1).to_numpy().ravel()\n",
    "    X_train = X_train[columns]\n",
    "    X_test = X_test[columns]\n",
    "    \n",
    "    if (len(X_train) < 1) or (len(X_test) < 1):\n",
    "        full_y = np.concatenate((y_train, y_test))\n",
    "        if len(full_y) < 1:\n",
    "            return agency_id, None, None\n",
    "        else:\n",
    "            empirical_probs = pd.Series(full_y).value_counts(normalize=True)\n",
    "            return agency_id, empirical_probs, None\n",
    "    \n",
    "    # Initialize the Random Forest model\n",
    "    random_forest = RandomForestClassifier(n_estimators=n_trees, max_depth=max_depth, random_state=seed, criterion='log_loss')\n",
    "    # random_forest = GradientBoostingClassifier(n_estimators=5, learning_rate=0.1, max_depth=7, random_state=seed)\n",
    "\n",
    "    # Train the Random Forest model on the training data\n",
    "    random_forest.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities for the testing data\n",
    "    probabilities = random_forest.predict_proba(X_test)\n",
    "\n",
    "    # Get the top n predicted classes for each sample\n",
    "    top_n_indices = np.argsort(probabilities, axis=1)[:, -top_n_classes:]\n",
    "\n",
    "    # Check if the true label is in the top n predicted classes for each sample\n",
    "    predicted_labels = random_forest.classes_[top_n_indices]\n",
    "    accurate_predictions = np.any(predicted_labels == y_test[:, np.newaxis], axis=1)\n",
    "\n",
    "    # Calculate accuracy based on whether the true label is in the top n predicted classes\n",
    "    accuracy = np.mean(accurate_predictions)\n",
    "    \n",
    "    return agency_id, random_forest, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f483bc0-07ff-4937-b6ad-2f486f400a74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agency_id, model, acc = train_forest(X_train, X_test, y_train, y_test, \n",
    "                                     agency_id='7008',\n",
    "                                     columns=final_cols, top_n_classes=top_n_classes,\n",
    "                                     n_trees=n_trees, max_depth=max_depth)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8fcf8c98-9f6b-4a91-bf34-c6e84b54ab6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_office_models(X_train, X_test, y_train, y_test, agencies, columns, top_n_classes, n_trees=1000, max_depth=2):\n",
    "    model_dic = {}\n",
    "    for agency in agencies:\n",
    "        agency_id, model, acc = train_forest(X_train, X_test, y_train, y_test, \n",
    "                                             agency_id=str(agency),\n",
    "                                             columns=final_cols, top_n_classes=top_n_classes,\n",
    "                                             n_trees=n_trees, max_depth=max_depth)\n",
    "        model_dic[agency_id] = [model, acc]\n",
    "    return model_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66b80f2d-10db-4dea-98af-736d7fffb915",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('7008',\n",
       " RandomForestClassifier(criterion='log_loss', max_depth=3, random_state=2000),\n",
       " 1.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_forest(X_train, X_test, y_train, y_test, \n",
    "                                             agency_id='7008',\n",
    "                                             columns=final_cols, top_n_classes=top_n_classes,\n",
    "                                             n_trees=n_trees, max_depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1d1750f4-5b2e-4835-8dd6-33bfbed1a4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Agency_ID_to_Name.pkl', 'rb') as f:\n",
    "        agency_id_to_name = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74176aff-a971-4b42-bde2-abc349f16f8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agencies = list(agency_id_to_name.keys())\n",
    "office_models = train_office_models(X_train, X_test, y_train, y_test,\n",
    "                                    agencies=agencies,\n",
    "                                    columns=final_cols,\n",
    "                                    top_n_classes=5,\n",
    "                                    n_trees=100,\n",
    "                                    max_depth=2)\n",
    "agencies_bool = [False if p[0] is None else True for p in list(office_models.values())]\n",
    "def filter_dict_by_bools(dictionary, bool_list):\n",
    "    filtered_dict = {}\n",
    "    for key, include in zip(dictionary.keys(), bool_list):\n",
    "        if include:\n",
    "            filtered_dict[key] = dictionary[key]\n",
    "    return filtered_dict\n",
    "office_models = filter_dict_by_bools(office_models, agencies_bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b188018f-fb62-4dd7-9a2c-86df5ef667a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Agency_for_Offices.pkl', 'rb') as f:\n",
    "        agency_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18236d4b-99d8-4f21-ae84-5fc1ca61c4d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('Class_IDs.pkl', 'rb') as f:\n",
    "        agency_class_to_id = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dc1b7a86-6d35-42b4-8c91-75b6459bfe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(row, y_row, agency_clf, offices_clf, top_n_results, top_n_offices):\n",
    "    row = row[final_cols]\n",
    "    agency_probs = pd.DataFrame(agency_clf.predict_proba(row)).T\n",
    "    agency_probs = agency_probs.reset_index()\n",
    "    agency_probs['Agency ID'] = agency_probs['index'].map(agency_class_to_id)\n",
    "    agency_probs.columns = ['Agency Label', 'Agency Probability', 'Agency ID']\n",
    "    \n",
    "    agency_probs['Office Model'] = agency_probs.apply(lambda x: offices_clf[x['Agency ID']] if x['Agency ID'] in list(offices_clf.keys()) else [None, 0] , axis=1)\n",
    "    \n",
    "    agency_probs = agency_probs.sort_values('Agency Probability', ascending=False).head(20)\n",
    "    office_dicts = []\n",
    "    for i in range(len(agency_probs)):\n",
    "        df_row = agency_probs.iloc[i]['Office Model']\n",
    "        if df_row[1] is None:\n",
    "            office_prob = df_row[0].to_list()\n",
    "            office_label = list(df_row[0].index)\n",
    "        elif df_row[0] is None:\n",
    "            office_prob = [0]\n",
    "            office_label = [\"ERROR\"]\n",
    "        else:\n",
    "            office_prob = df_row[0].predict_proba(row)[0].tolist()\n",
    "            office_label = df_row[0].classes_.tolist()\n",
    "    \n",
    "        office_dicts += [{k: v for k, v in zip(office_label, office_prob)}]\n",
    "    \n",
    "    office_dicts_new = []\n",
    "    for dic in office_dicts:\n",
    "        office_dicts_new += [[{k: v} for k, v in dic.items()]]\n",
    "    \n",
    "    agency_probs['Office Dict'] = office_dicts_new\n",
    "    total_prob = agency_probs.explode('Office Dict')\n",
    "    total_prob['Office Label'] = total_prob['Office Dict'].apply(lambda x: list(x.keys())[0])\n",
    "    office_label_to_id = {k: v for k, v in zip(range(len(class_ids)), class_ids)}\n",
    "    total_prob['Office ID'] = total_prob['Office Label'].map(office_label_to_id)\n",
    "    total_prob['Office Probability'] = total_prob['Office Dict'].apply(lambda x: list(x.values())[0])\n",
    "    total_prob['Agency_Office Probability'] = total_prob['Agency Probability'] * total_prob['Office Probability']\n",
    "    \n",
    "    # top_n = total_prob.sort_values('Agency_Office Probability', ascending=False).head(top_n_results)\n",
    "    # office_result = np.any(top_n['Office Label'] == y_row['RESPONSE'])\n",
    "    # agency_result = np.any(top_n['Agency ID'] == y_row['FUNDING_AGENCY_ID'])\n",
    "    \n",
    "    uniform_prob = 1 / total_prob.groupby('Agency Label').size().to_frame()\n",
    "    merged_prob = total_prob.merge(uniform_prob, left_on='Agency Label', right_on=uniform_prob.index)\n",
    "    merged_prob = merged_prob[merged_prob['Office Probability'] >= merged_prob[0]] #gotta be at least better than uniform to make it\n",
    "    \n",
    "    merged_prob['Agency_Office Probability'] = merged_prob['Agency Probability'] * merged_prob['Office Probability']\n",
    "    \n",
    "    # def get_top_n_rows(group): #get only top_n offices for each agency, also normalize probability based on new subset\n",
    "    #     top_n = group.nlargest(top_n_offices, 'Office Probability')\n",
    "    #     top_n['Office Proportion'] = top_n['Office Probability'] / top_n['Office Probability'].sum()\n",
    "    #     return top_n\n",
    "    # merged_prob = merged_prob.groupby('Agency Label', group_keys=False).apply(get_top_n_rows, include_groups=False)\n",
    "    # merged_prob['Agency_Office Probability'] = merged_prob['Agency Probability'] * merged_prob['Office Proportion']\n",
    "    \n",
    "    top_n = merged_prob.sort_values(['Agency_Office Probability'], ascending=False).head(top_n_results)\n",
    "    office_result = np.any(top_n['Office Label'] == y_row['RESPONSE'])\n",
    "    agency_result = np.any(top_n['Agency ID'] == y_row['FUNDING_AGENCY_ID'])    \n",
    "    \n",
    "    return office_result, agency_result, merged_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cd5c8774-195c-4777-885c-d372016876d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# office_result, agency_result = predict(pd.DataFrame(X_test.iloc[5]).T, y_test.iloc[5], agency_clf, office_models, top_n_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ee87cac4-b51c-48cb-8507-901fd2f9948d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 24 minutes to run through all of them (not limiting agency probabilities)\n",
    "## 9 minutes for all (top 40 agencies only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78bd14f8-561f-4eff-98f8-197f057123bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "office_results = []\n",
    "agency_results = []\n",
    "for i in range(len(X_test)):\n",
    "    office_result, agency_result, merged_prob = predict(pd.DataFrame(X_test.iloc[i]).T, y_test.iloc[i],\n",
    "                                                        agency_clf, office_models, top_n_results=20, top_n_offices=20)\n",
    "    office_results += [office_result]\n",
    "    agency_results += [agency_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9a970fb4-a4fc-4ec9-814e-0724137f4eed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5199858507251504"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(office_results) / len(office_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "79bcd099-a5d1-480d-a4a7-6fd1a6e28cf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8142907675981605"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(agency_results) / len(agency_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7640bca1-0487-4ffd-a67c-0c8f8d852776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# better than uniform cutoff & now min number of contracts per year is 8\n",
    "# 0.5199858507251504\n",
    "# 0.8142907675981605\n",
    "# better than uniform cutoff & now min number of contracts per year is 8 (top 10 offices per agency)\n",
    "# 0.5129112133003184\n",
    "# 0.8135833038556773\n",
    "# same as above w/ 20 offices per agency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8942c94a-d0a8-4366-900d-096b45a5cb82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# added the how many times have you contracted this year variable:\n",
    "# top 20 office when agency limited to top 20: 0.5008798592225244\n",
    "# top 20 agency when agency limited to top 20: 0.818109102543593\n",
    "# require better than uniform office prob to make it:\n",
    "# top 20 office when agency limited to top 20: 0.5018397056470965\n",
    "# top 20 agency when agency limited to top 20: 0.8246680531115022\n",
    "# when you do only top 10 offices per agency:\n",
    "# 0.47528395456726924\n",
    "# 0.8027515597504399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9517a1f1-4df1-4665-92a8-22c1af4195b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgboosted agency model\n",
    "# top 20 office when agency limited to top 20: 0.3469844824828027\n",
    "# top 20 agency when agency limited to top 20: 0.5155975043992961\n",
    "# when you add the filtering for only better than uniform offices:\n",
    "# 0.3519436890097584\n",
    "# 0.5519116941289394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c863d4f5-65ff-406e-83a2-aa592bf9941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.001, log_loss on agency & office\n",
    "# top 20 office when agency limited to top 20: 0.40889457686770114\n",
    "# top 20 agency when agency limited to top 20: 0.7398816189409695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818ced4-a2e0-420b-a104-c9ac2dd5a1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.001 better for column selection\n",
    "# this cell uses 0.001 and does weighted averages instead of probabilities (80% agency, 20% office)\n",
    "# top 20 office when agency limited to top 40: 0.3258678611422172\n",
    "# top 20 agency when agency limited to top 40: 0.48616221404575266"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f02ee82-882d-4b89-b1bb-a95b81ebe3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but (20% agency, 80% office)\n",
    "# top 20 office when agency limited to top 20: 0.2839545672692369\n",
    "# top 20 agency when agency limited to top 20: 0.46168613021916494"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f72120-50ca-4477-a749-abd6ec04f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these cells use a cutoff of 0.001 for column selection\n",
    "# top 20 office when agency limited to top 40: 0.4053751399776036\n",
    "# top 20 agency when agency limited to top 40: 0.7306031035034395\n",
    "# top 20 office when agency limited to top 20: 0.40457526795712684\n",
    "# top 20 agency when agency limited to top 20: 0.7358822588385858"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5a6593-bc48-4086-b8be-ce99ad047aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the below cells all use a cutoff of 0.015 for column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9ef86-0cbb-4250-8287-cfe8e0c0670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 20 office when agency limited to top 40: 0.39529675251959684\n",
    "# top 20 agency when agency limited to top 40: 0.710926251799712\n",
    "# top 20 office when agency limited to top 20: 0.3922572388417853\n",
    "# top 20 agency when agency limited to top 20: 0.7149256119020957"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f80767-6df8-4136-bd27-cc41108c1af1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top 20: 0.39529675251959684\n",
    "# top 20 when I limit agency to top 40: 0.39529675251959684\n",
    "# top 20 when I limit agency to top 20: 0.3922572388417853\n",
    "# top 10 when I limit agency to top 20: 0.3092305231163014\n",
    "# top 20 when I limit agency to top 10: 0.3725803871380579"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
