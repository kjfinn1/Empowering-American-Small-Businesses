{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab310def-33d0-42d0-822a-abb60290e2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dc8d67c-489d-4b6a-9b1b-534d5a2377e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import_years = [2019, 2020, 2021, 2022, 2023]\n",
    "import_cols = [\n",
    "    'CO_BUS_SIZE_DETERMINATION',\n",
    "    'CAGE_CODE',\n",
    "    'FUNDING_AGENCY_NAME',\n",
    "    'FUNDING_AGENCY_ID',\n",
    "    # 'FUNDING_OFFICE_NAME',\n",
    "    # 'FUNDING_OFFICE_ID',\n",
    "    'VENDOR_ADDRESS_COUNTRY_NAME',\n",
    "    'EXTENT_COMPETED',\n",
    "    'DOLLARS_OBLIGATED',\n",
    "    'SOLICITATION_ID'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f972ab63-0043-4d55-a886-d7356312e614",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def import_dataset(import_cols, years, sam=True):\n",
    "    \"\"\"\n",
    "    Imports, cleans, and joins our data with specified columns and years\n",
    "    Inputs: \n",
    "        import_cols (list [str] of column names)\n",
    "        years (list [int] of years to import)\n",
    "        sam (bool of whether to merge with sam dataset)\n",
    "    Output:\n",
    "        Cleaned, filtered, and joined dataframe\n",
    "    \"\"\"\n",
    "    if sam:\n",
    "        SAM = pd.read_csv('SAM.CSV') #imports SAM df\n",
    "    \n",
    "    year_dfs = []\n",
    "    for year in years:\n",
    "        temp_df = pd.read_parquet(str(year) + '.parquet', columns=import_cols) #import year's data\n",
    "        \n",
    "        temp_df = temp_df[temp_df['CO_BUS_SIZE_DETERMINATION'] == \"SMALL BUSINESS\"] #filter for small business\n",
    "        temp_df = temp_df[temp_df['VENDOR_ADDRESS_COUNTRY_NAME'] == \"UNITED STATES\"] #filter for US\n",
    "        temp_df = temp_df[temp_df['EXTENT_COMPETED'].isin([\"A\", \"D\", \"E\", \"CDO\"])] #filter for competition\n",
    "        \n",
    "        # temp_df['DOLLARS_OBLIGATED'] = pd.to_numeric(temp_df['DOLLARS_OBLIGATED'], errors='coerce') #make numeric\n",
    "        \n",
    "        if sam:\n",
    "            temp_m = pd.merge(temp_df, SAM, on=\"CAGE_CODE\", how=\"inner\") #merge with SAM\n",
    "        else:\n",
    "            temp_m = temp_df\n",
    "        \n",
    "        # idx = temp_m.groupby(['SOLICITATION_ID','CAGE_CODE'])['DOLLARS_OBLIGATED'].idxmax() #find initial contract win\n",
    "        # temp_m = temp_m.loc[idx] #filter to initial contract win\n",
    "        \n",
    "        temp_m = temp_m[temp_m['DOLLARS_OBLIGATED'] > 0] #filter DOLLARS_OBLIGATED\n",
    "        \n",
    "        print(f'{year} shape: {temp_m.shape}')\n",
    "        year_dfs.append(temp_m) #append year dataset to list of year datasets\n",
    "    \n",
    "    merged_df = pd.concat(year_dfs, ignore_index=True) #merge all years\n",
    "    \n",
    "    for df in year_dfs:\n",
    "        del df\n",
    "    del year_dfs #delete the individual dfs from memory\n",
    "    \n",
    "    idx = merged_df.groupby(['SOLICITATION_ID','CAGE_CODE'])['DOLLARS_OBLIGATED'].idxmax() #find initial contracts\n",
    "    filtered_merged_df = merged_df.loc[idx] #filter to initial contract\n",
    "    \n",
    "    print(f'total shape: {filtered_merged_df.shape}')\n",
    "    \n",
    "    #place of manufacture conversion\n",
    "    def convert_place_of_manufacture(value):\n",
    "        if value == 'D':\n",
    "            return 'YES' #manufactured in US\n",
    "        elif value == 'C':\n",
    "            return 'NO' #not manufactured in US\n",
    "        elif value in ['N/A', 'A', 'G', 'E', 'H', 'L', 'J', 'F', 'K', 'B', 'I']:\n",
    "            return 'NONE'\n",
    "        else:\n",
    "            return 'NONE' #N/A (provides a service or doesn't qualify fully)\n",
    "    \n",
    "    #clean up individual columns\n",
    "    filtered_merged_df['FUNDING_AGENCY_NAME'] = filtered_merged_df['FUNDING_AGENCY_NAME'].str.strip()\n",
    "    filtered_merged_df = filtered_merged_df.dropna(subset=filtered_merged_df.columns.difference(['PLACE_OF_MANUFACTURE'])) #remove rows with NAs here\n",
    "    \n",
    "    print(f'total filtered shape: {filtered_merged_df.shape}')\n",
    "    \n",
    "    return filtered_merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ac86b18-3ee9-4549-9c19-a7d0fb74a331",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019 shape: (1772611, 8)\n",
      "2020 shape: (1659942, 8)\n",
      "2021 shape: (1916684, 8)\n",
      "2022 shape: (1979453, 8)\n",
      "2023 shape: (1866515, 8)\n",
      "total shape: (125915, 8)\n",
      "total filtered shape: (125915, 8)\n"
     ]
    }
   ],
   "source": [
    "df0 = import_dataset(import_cols, import_years, sam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "79780543-c87a-426e-b9ef-cf8f92f52d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "id_name_list = list(df0.groupby(['FUNDING_AGENCY_ID', 'FUNDING_AGENCY_NAME']).size().index)\n",
    "agency_id_to_name = {i[0]: i[1] for i in id_name_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f518f86e-a9e2-4cbe-9972-7d34a78b492d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'0061'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m agency_id_to_name[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0061\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: '0061'"
     ]
    }
   ],
   "source": [
    "agency_id_to_name['0061']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8aecb478-0d03-4ba0-ba5d-407099e56518",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pickle.dump(agency_id_to_name, open('Agency_ID_to_Name.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2066b740-38d8-4b92-9a03-49e862f2c90e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNITED STATES GOVERNMENT PUBLISHING OFFICE'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agency_id_to_name['0400']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
